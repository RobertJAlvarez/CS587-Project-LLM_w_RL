{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb0ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvar258/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1de4dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Total samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Download dataset.\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "openwebtext = load_dataset(\"stas/openwebtext-10k\")\n",
    "\n",
    "texts = [sample[\"text\"] for sample in openwebtext[\"train\"] if \"text\" in sample]\n",
    "\n",
    "print(f\"Total samples: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d23fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from all samples.\n",
    "prompts = [text.split(\".\") for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79b730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min=1, max=1736, avg=44.5757\n"
     ]
    }
   ],
   "source": [
    "n_sentences = [len(prompt) for prompt in prompts]\n",
    "\n",
    "print(f\"min={min(n_sentences)}, max={max(n_sentences)}, avg={sum(n_sentences)/len(n_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b91f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower 10 freq: [(1, 10), (2, 4), (3, 6), (4, 12), (5, 54), (6, 88), (7, 130), (8, 179), (9, 217), (10, 264)]\n",
      "Top 10 freq: [(768, 1), (821, 1), (860, 1), (926, 1), (944, 1), (954, 1), (968, 1), (1003, 1), (1020, 1), (1736, 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Print number of prompts that only have 0-~10 sentences in it.\n",
    "freq_table = dict(Counter(n_sentences)).items()\n",
    "print(f\"Lower 10 freq: {sorted(freq_table)[:10]}\")\n",
    "print(f\"Top 10 freq: {sorted(freq_table)[-10:]}\")\n",
    "\n",
    "# NOTE: Dropping prompts with <= 3 sentences would result in 0.2% of prompts drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a26eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_sentences = [\n",
    "    sample[\"text\"].split(\".\") for sample in openwebtext[\"train\"] if \"text\" in sample\n",
    "][:10]\n",
    "\n",
    "prompts = []\n",
    "references = []\n",
    "for sentences in prompts_sentences:\n",
    "    # Don't use single sentece prompts.\n",
    "    if len(sentences) == 1:\n",
    "        continue\n",
    "    # Use from 1-10 sentences as input.\n",
    "    n_sentences = len(sentences)\n",
    "    idx = n_sentences - 1 if n_sentences < 10 else 10\n",
    "    prompts.append(\". \".join(sentences[:idx]) + \".\")\n",
    "    references.append(prompts[-1] + \" \" + sentences[idx] + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d628a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 33, 21, 25, 15, 56, 14, 44, 9, 17]\n"
     ]
    }
   ],
   "source": [
    "print([len(sentences) for sentences in prompts_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec85624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_dots = 10 =? 10\n",
      "n_dots = 11 =? 11\n",
      "n_dots = 8 =? 8\n",
      "n_dots = 9 =? 9\n"
     ]
    }
   ],
   "source": [
    "# Original sentence had 41 periods.\n",
    "t = prompts[0].count(\".\")\n",
    "print(f\"n_dots = {t} =? 10\")\n",
    "t = references[0].count(\".\")\n",
    "print(f\"n_dots = {t} =? 11\")\n",
    "\n",
    "# Original sentence had 9 periods.\n",
    "t = prompts[8].count(\".\")\n",
    "print(f\"n_dots = {t} =? 8\")\n",
    "t = references[8].count(\".\")\n",
    "print(f\"n_dots = {t} =? 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b452af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts_and_references(train_pct: int = 80, test_pct: str = 20):\n",
    "    \"\"\"\n",
    "    Load prompts and references from OpenWebText.\n",
    "    Split into training and test sets.\n",
    "    \"\"\"\n",
    "    tot_pct = train_pct + test_pct\n",
    "    assert tot_pct <= 100, \"Train + test percent > 100\"\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    openwebtext = load_dataset(\"stas/openwebtext-10k\")\n",
    "\n",
    "    # Extract each prompt and split by sentence.\n",
    "    prompts_sentences = [\n",
    "        sample[\"text\"].split(\".\") for sample in openwebtext[\"train\"] if \"text\" in sample\n",
    "    ]\n",
    "\n",
    "    print(f\"Total samples: {len(prompts_sentences)}\")\n",
    "\n",
    "    prompts = []\n",
    "    references = []\n",
    "    for sentences in prompts_sentences:\n",
    "        # Don't use single sentece prompts.\n",
    "        if len(sentences) <= 1:\n",
    "            continue\n",
    "        # Use from 1-10 sentences as prompt.\n",
    "        n_sentences = len(sentences)\n",
    "        idx = n_sentences - 1 if n_sentences <= 7 else 7\n",
    "        next_sentence_words = sentences[idx].lstrip().split(\" \")\n",
    "        if len(next_sentence_words) <= 1:\n",
    "            continue\n",
    "        prompts.append(\". \".join(sentences[:idx]) + \". \" + next_sentence_words[0] + \" \")\n",
    "        references.append(prompts[-1] + \" \".join(next_sentence_words[1:]) + \".\")\n",
    "\n",
    "    # Split.\n",
    "    train_size = int((train_pct / tot_pct) * len(prompts))\n",
    "\n",
    "    train_prompts = prompts[:train_size]\n",
    "    train_references = references[:train_size]\n",
    "    test_prompts = prompts[train_size:]\n",
    "    test_references = references[train_size:]\n",
    "\n",
    "    print(\n",
    "        f\"Loaded {len(train_prompts)} training samples and {len(test_prompts)} test samples.\"\n",
    "    )\n",
    "    return (train_prompts, train_references), (test_prompts, test_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eead497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Total samples: 10000\n",
      "Loaded 7336 training samples and 1835 test samples.\n"
     ]
    }
   ],
   "source": [
    "train, test = load_prompts_and_references()\n",
    "train_prompts, train_references = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ddf43d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Prompt **\n",
      "Don’t raise your voice here. \n",
      "\n",
      "Angela Kabari Blocked Unblock Follow Following Jul 20, 2017\n",
      "\n",
      "My name is Angela.  I am the woman in the centre of the current Ushahidi sexual harassment scandal. \n",
      "\n",
      "The past six months have been some of the most bizarre in my life and, on the balance, I think there is benefit in sharing my experience with the world so that lessons may be learned from it.  It is my hope that, my story shall prompt a change in company policies, both in the Kenyan tech space and in other fields. \n",
      "\n",
      "I joined Ushahidi in September 2015 as a Capacity Development Officer for Making All Voices Count.  My time there was mostly enjoyable: the work was challenging, the team was great, and the environment was liberal and progressive. All \n",
      "** Reference **\n",
      "Don’t raise your voice here. \n",
      "\n",
      "Angela Kabari Blocked Unblock Follow Following Jul 20, 2017\n",
      "\n",
      "My name is Angela.  I am the woman in the centre of the current Ushahidi sexual harassment scandal. \n",
      "\n",
      "The past six months have been some of the most bizarre in my life and, on the balance, I think there is benefit in sharing my experience with the world so that lessons may be learned from it.  It is my hope that, my story shall prompt a change in company policies, both in the Kenyan tech space and in other fields. \n",
      "\n",
      "I joined Ushahidi in September 2015 as a Capacity Development Officer for Making All Voices Count.  My time there was mostly enjoyable: the work was challenging, the team was great, and the environment was liberal and progressive. All in all, a good place to be.\n"
     ]
    }
   ],
   "source": [
    "print(\"** Prompt **\")\n",
    "print(train_prompts[10])\n",
    "\n",
    "print(\"** Reference **\")\n",
    "print(train_references[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384dde3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
