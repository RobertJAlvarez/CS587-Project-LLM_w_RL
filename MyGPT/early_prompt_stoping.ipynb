{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf8bbb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datasets import load_dataset  # type: ignore\n",
    "import json\n",
    "import nltk  # type: ignore\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  # type: ignore\n",
    "import random\n",
    "import tiktoken  # type: ignore\n",
    "import time\n",
    "import torch  # type: ignore\n",
    "import torch.nn as nn  # type: ignore\n",
    "import torch.optim as optim  # type: ignore\n",
    "\n",
    "from my_gpt import GPT, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a36c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.5,\n",
    "    top_k=10,\n",
    "    use_kv_cache=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from a prompt using the trained GPT model with KV caching support.\n",
    "\n",
    "    Args:\n",
    "        model: The trained GPT model.\n",
    "        tokenizer: The tokenizer used to encode/decode text.\n",
    "        prompt: The text prompt to start generation.\n",
    "        max_new_tokens: Maximum number of tokens to generate.\n",
    "        temperature: Controls randomness (higher = more random).\n",
    "        top_k: Number of highest probability tokens to consider for sampling.\n",
    "        use_kv_cache: Whether to use KV caching for more efficient generation.\n",
    "\n",
    "    Returns:\n",
    "        The generated text including the prompt and generation time.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "\n",
    "    # Encode the prompt.\n",
    "    encoded_prompt = tokenizer.encode(prompt)\n",
    "    tokens = (\n",
    "        torch.tensor(encoded_prompt, dtype=torch.long)\n",
    "        .unsqueeze(0)\n",
    "        .to(model.lm_head.weight.device)\n",
    "    )\n",
    "\n",
    "    # Track timing for performance analysis.\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize the past key values to None (no caching yet).\n",
    "    past_key_values = None\n",
    "    max_past_key_values_len = model.config.block_size - 1\n",
    "\n",
    "    # Generate tokens one at a time.\n",
    "    for _ in range(max_new_tokens):\n",
    "        # For KV cache: after first iteration, only process the last token.\n",
    "        # For no KV cache: always process full sequence within block size limit.\n",
    "        if not use_kv_cache or past_key_values is None:\n",
    "            # Get only the last block_size tokens if input is too long.\n",
    "            context = tokens[:, -model.config.block_size :]\n",
    "        else:\n",
    "            context = tokens[:, -1:]  # With KV cache, we only need the last token.\n",
    "            # Get only the last block_size - 1 KV cache if the total input (KV cache + context) is too long.\n",
    "            if past_key_values[0][0].size(2) > max_past_key_values_len:\n",
    "                past_key_values = list(\n",
    "                    tuple(t[:, :, -max_past_key_values_len:] for t in layer_past)\n",
    "                    for layer_past in past_key_values\n",
    "                )\n",
    "\n",
    "        # Forward pass to get logits.\n",
    "        with torch.no_grad():\n",
    "            logits, new_past_key_values = model(\n",
    "                context, past_key_values=past_key_values\n",
    "            )\n",
    "\n",
    "            # Update KV cache for next iteration if using cache.\n",
    "            if use_kv_cache:\n",
    "                past_key_values = new_past_key_values\n",
    "\n",
    "        # Focus on the last token's predictions.\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        # Apply top-k filtering.\n",
    "        if top_k > 0:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            # Set other logits outside top-l to a value of -inf.\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "        # Apply softmax to get probabilities.\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample from the distribution.\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # TODO: Add \".\" as another token for early stoping.\n",
    "\n",
    "        # If we reach the end of text token, stop.\n",
    "        if next_token.item() == tokenizer.eot_token:\n",
    "            break\n",
    "        else:\n",
    "            # Append the token to our sequence.\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "    # Calculate generation time.\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    # Decode the tokens.\n",
    "    generated_text = tokenizer.decode(tokens[0].tolist())\n",
    "\n",
    "    # Return both the generated text and timing information.\n",
    "    return generated_text, generation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c7f47",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_fluency_score(text):\n",
    "    \"\"\"\n",
    "    A simple proxy for fluency: longer sentences, proper punctuation.\n",
    "    (Optional improvement: use perplexity from a small language model.)\n",
    "    \"\"\"\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(\n",
    "        sentences\n",
    "    )\n",
    "\n",
    "    # Normalize to a range, e.g., divide by 20 (typical sentence length).\n",
    "    return min(avg_sentence_length / 20.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd05499",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer (GPT-4 tokenizer).\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Set device.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model configuration (must match the trained model's configuration).\n",
    "config = Config(\n",
    "    vocab_size=tokenizer.n_vocab, n_embd=256, n_head=8, n_layer=4, block_size=128\n",
    ")\n",
    "\n",
    "# Initialize the model.\n",
    "model = GPT(config)\n",
    "\n",
    "# Load the trained weights.\n",
    "try:\n",
    "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "    print(f\"Model loaded from {args.model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Move model to the appropriate device.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffa6ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate text using specified setting.\n",
    "print(\n",
    "    f\"\\nGenerating text using {'KV cache' if args.use_kv_cache else 'standard generation'}...\"\n",
    ")\n",
    "\n",
    "generated_text, generation_time = generate(\n",
    "    model=model,\n",
    "    prompt=args.prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=args.max_tokens,\n",
    "    temperature=args.temperature,\n",
    "    top_k=args.top_k,\n",
    "    use_kv_cache=args.use_kv_cache,\n",
    ")\n",
    "\n",
    "# Print timing information.\n",
    "print(f\"Generation completed in {generation_time:.4f} seconds\")\n",
    "\n",
    "# Print the generated text.\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(\"-\" * 50)\n",
    "print(generated_text)\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
